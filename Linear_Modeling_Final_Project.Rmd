---
title: "Linear_Modeling_Final_Project"
author: "STEPHEN MEGOS EAST"
date: "4/12/2021"
output: pdf_document
---

## Part I

```{r}
library(tidyverse)
library(rms)
library(MASS)
library(dplyr)
library(lmtest)
library(faraway)
set.seed(1)
```

```{r}
crimes <- read.csv('CommunitiesAndViolence.txt', header=T)

colnames <- c("communityname", "State", "countyCode", "communityCode", "fold", 
              "pop", "perHoush", "pctBlack", "pctWhite", "pctAsian", "pctHisp", 
              "pct12-21", "pct12-29", "pct16-24", "pct65up", "persUrban", "pctUrban", 
              "medIncome", "pctWwage", "pctWfarm", "pctWdiv", "pctWsocsec", 
              "pctPubAsst", "pctRetire", "medFamIncome", "perCapInc", "whitePerCap",
              "blackPerCap", "NAperCap", "asianPerCap", "otherPerCap", "hispPerCap",
              "persPoverty", "pctPoverty", "pctLowEdu", "pctNotHSgrad", "pctCollGrad",
              "pctUnemploy", "pctEmploy", "pctEmployMfg", "pctEmployProfServ",
              "pctOccupManu", "pctOccupMgmt", "pctMaleDivorc", "pctMaleNevMar",
              "pctFemDivorc", "pctAllDivorc", "persPerFam", "pct2Par", "pctKids2Par",
              "pctKids-4w2Par", "pct12-17w2Par", "pctWorkMom-6", "pctWorkMom-18",
              "kidsBornNevrMarr", "pctKidsBornNevrMarr", "numForeignBorn", "pctFgnImmig-3",
              "pctFgnImmig-5", "pctFgnImmig-8", "pctFgnImmig-10", "pctImmig-3",
              "pctImmig-5", "pctImmig-8", "pctImmig-10", "pctSpeakOnlyEng",
              "pctNotSpeakEng", "pctLargHousFam", "pctLargHous", "persPerOccupHous",
              "persPerOwnOccup", "persPerRenterOccup", "pctPersOwnOccup",
              "pctPopDenseHous", "pctSmallHousUnits", "medNumBedrm", "houseVacant",
              "pctHousOccup", "pctHousOwnerOccup", "pctVacantBoarded", "pctVacant6up",
              "medYrHousBuilt", "pctHousWOphone", "pctHousWOplumb", "ownHousLowQ",
              "ownHousMed", "ownHousUperQ", "ownHousQrange", "rentLowQ", "rentMed",
              "rentUpperQ", "rentQrange", "medGrossRent", "medRentpctHousInc",
              "medOwnCostpct", "medOwnCostPctWO", "persEmergShelt", "persHomeless",
              "pctForeignBorn", "pctBornStateResid", "pctSameHouse-5", "pctSameCounty-5",
              "pctSameState-5", "numPolice", "policePerPop", "policeField",
              "policeFieldPerPop", "policeCalls", "policCallPerPop", "policCallPerOffic",
              "policePerPop2", "racialMatch", "pctPolicWhite", "pctPolicBlack",
              "pctPolicHisp", "pctPolicAsian", "pctPolicMinority", "officDrugUnits",
              "numDiffDrugsSeiz", "policAveOT", "landArea", "popDensity", "pctUsePubTrans",
              "policCarsAvail", "policOperBudget","pctPolicPatrol", "gangUnit",
              "pctOfficDrugUnit", "policBudgetPerPop", "murders", "murdPerPop", "rapes",
              "rapesPerPop", "robberies", "robbbPerPop", "assaults", "assaultPerPop",
              "burglaries", "burglPerPop", "larcenies", "larcPerPop", "autoTheft",
              "autoTheftPerPop", "arsons", "arsonsPerPop","violentPerPop",
              "nonViolPerPop")

names(crimes) <- colnames
```
Plot 1: Burglaries as a function of population
```{r}
crimes$burglaries <- (as.numeric(crimes$burglaries))
crimes <- na.omit(crimes)

crimes$pop <- (as.numeric(crimes$pop))
crimes <- na.omit(crimes)

ggplot(crimes, 
       aes(y=crimes$burglaries,
           x=pop)) + 
  xlab('Population') + 
  ylab('Burglaries') +
  ggtitle('Burglaries as a Function of Population') +
  geom_point()
```

According to the graph above, communities with higher populations are more likely to have greater numbers of burglaries.

Plot 2: Burglaries as a function of Median Income 
```{r}
ggplot(crimes, 
       aes(y=crimes$burglaries,
           x=medIncome)) + 
  xlab('Median Income') + 
  ylab('Burglaries') +
  ggtitle('Burglaries as a Function of Median Income') +
  geom_point()
```

According to the second graph, communities with lower median income, have a higher rate of burglaries. I would have assumed communities with higher median incomes would have greater numbers of burglaries.

Plot 3: Burglaries as a function of number of people identified as homeless
```{r}
ggplot(crimes, 
       aes(y=crimes$burglaries,
           x=persHomeless)) + 
  xlab('Number of Homeless Persons') + 
  ylab('Burglaries') +
  ggtitle('Burglaries as a Function of Number of Homeless Persons') +
  geom_point()
```

According to the third graph, as police operating budgets increase, the communities that they police experience greater rates of burglaries

Plot 4: Burglaries as a function of Racial Match
```{r}
crimes$racialMatch <- (as.numeric(crimes$racialMatch))
crimes <- na.omit(crimes)

ggplot(crimes, 
       aes(y=crimes$burglaries,
           x=racialMatch)) + 
  xlab('Racial Match') + 
  ylab('Burglaries') +
  ggtitle('Burglaries as a Function of Racial Match') +
  geom_point()
```

In graph 4, the graph shows that communities with more of a match in race between the communities and police, show higher numbers of burglaries.


##Part II

```{r}
mod <- lm(burglaries~pop+medIncome+persHomeless+racialMatch, data = crimes)
summary(mod)
```

Written Answer:
Judging from the summary output above, racial match is not as critical of a value when determining the number of burglaries in an area, relative to the other factors. The most critical factor appears to be population which obviously makes sense. Other than population, median income appears to be the most critical out of the remaining three. After looking at the scatter plots I made for step 1, I assumed that population would be the most correlated factor, especially when compared to median income. But after looking at the model summary, it appears that because the p-value for median income shows that it is more correlated to burglaries than police operating budget.


## Part III

```{r}
# I had to change police operating budget out because the fastbw() function
  # wouldn't run as long as the policeoperating was present. It was throwing
  # an error that I was unfamiliar with
crimes <- na.omit(crimes)
ols.mod <- ols(burglaries~pop+medIncome+persHomeless+racialMatch, data = crimes)

fastbw(ols.mod, rule='p', sls=0.05)
# According to the fastbw() function, all of the variables should be kept in the model
  # I was expecting at least one of the variables to be irrelevant according to the
  # fastbw(). Specifically, the medIncome() variable, but after looking at the plot
  # and the pval, it does look correct and I think my intuition was incorrect.
```

Do StepAIC
```{r}
aic <- stepAIC(mod)
aic$anova
# According to the stepAIC and the automatically created model, the model is the
  # same as the one I created originally as part of step 2, which was derived from
  # the data created in step 1.
summary(mod)
# For step 4, I will use the automatically created model with the slightly altered
  # variable content since it seems to run better in R.
```


## Part IV

```{r}
final_mod <- lm(burglaries~pop+medIncome+persHomeless+racialMatch, data=crimes)

ggplot(final_mod, 
       aes(y=final_mod$residuals,
           x=final_mod$fitted.values)) + 
  xlab('Fitted Values') + 
  ylab('Residuals') +
  ggtitle('Diagnostic Plot') +
  geom_point()

# The fitted vs residuals plot doesn't really show a good result. I would have expected 
  # the graph to appear more spread out and evenly distributed along a line.
```

```{r}
qqnorm(final_mod$residuals)

# Here the QQ plot is very clearly non-linear with big curves indicating that 
  # the model assumption of normal errors IS NOT upheld
```

```{r}
n <- length(final_mod$residuals)
plot(tail(final_mod$residuals, n-1) ~ head(final_mod$residuals, n-1), 
     xlab = expression(hat(epsilon)[i]), 
     ylab=expression(hat(epsilon)[i+1]), 
     main = 'Lagged Residual Plot')

abline(h = 0, v = 0)

# According to the textbook, in this third plot, since the points are very visibly
  # clumped together, this contradicts the other two plots. If the errors were
  # uncorrelated, then we would have expected a much more spread out, and scattered
  # series of points. This plot indicates positive serial correlation.
```

## Part V

Question 1: Check for outliers and influential observations.
```{r}
# Checking for outliers in the 'final model'
rstandard <- data.frame(rstandard(final_mod))
glimpse(rstandard)

sum(abs(rstandard)>3)
which(abs(rstandard)>3)

plot(final_mod)
```

```{r}
n <- nrow(crimes)
p <- length(final_mod$coefficients)
fthresh <- qf(0.5, p, n-p)
fthresh

summary(cooks.distance(final_mod))

which(cooks.distance(final_mod)>fthresh)
which(cooks.distance(final_mod)>1)
```


Question 2: Write up brief explanations as to what you are seeing.
According to the rstandard() function, which calculates the residuals of the final model determined earlier, we can see how likely a value is to being an outlier. According to the output, the largest (absolute) residual value is ~1.5. In order to 'qualify' for being an outlier, a value needs to be around or at least, abs(3.0). According to these outputs there are 10 points that are considered to be outliers.

After determining the f statistic threshold, we can use that benchmark to look for any points that are considered 'influential' by comparing the model's 'cook's distance' variables to the f statistic threshold. After comparing the values, there is 1 value that exceeds the f threshold of 0.872. Some people may use a standard benchmark/threshold of 1, rather than calculating the f statistic threshold. When using 1 as the benchmark instead of 0.872, that same data point is also considered an influential point as well.

## Part VI

Question 1: Correct the model if mathematical assumptions of the model were not met in Step 4.
```{r}
# If a transformation is needed, indicate what measures you are taking 
# (i.e., Box-Cox transformation, polynomial regression, or some method that 
# wasn't covered if you're feeling adventurous).

mod <- final_mod
summary(mod)
# Diagnostic Test
x <- mod$residuals
y <- mod$fitted.values
plot(y,x)
# Breusch-Pagan Test
bptest(mod)
# Boxcox plot
bc <- boxcox(mod,plotit=T)

lambda <- bc$x[which.max(bc$y)]
lambda
```

```{r}
mod2 <- lm(burglaries^lambda~pop+medIncome+persHomeless+racialMatch,data=crimes)
summary(mod2)
# Diagnostic Test
x2 <- mod2$residuals
y2 <- mod2$fitted.values

ggplot(mod2, 
       aes(y=mod2$residuals,
           x=mod2$fitted.values)) + 
  xlab('Fitted Values') + 
  ylab('Residuals') +
  ggtitle('Diagnostic Plot After Box-Cox Transformation') +
  geom_point()

# Breusch-Pagan Test
bptest(mod2)
# Boxcox plot
bc2 <- boxcox(mod2,plotit=T)

lambda2 <- bc$x[which.max(bc2$y2)]
lambda2
```

Written Answer: According to the original model, it looks like maybe there might be a bit of a cone-shaped but nothing significant. According to the boxcox method and looking at the lambda, the values are clearly not centered around 0. So, I used an alternate method and after looking at the re-structured residuals/fitted values plot, it appears clear to me that the plot is more varied and more random appearing.


## Part VII

Question 1: Report the final model and use it to perform inferences
```{r}
# Part 1
 # Final model inferences
data.frame(summary(mod2)$coefficients)
```

```{r}
# Part 2
 # R squared of the model
summary(mod2)$r.squared
```

```{r}
# Part 3
  # Compute Confidence Interval
(coef(summary(mod2))[, 'Estimate']['medIncome']) + (qt(.975, 45)) * (coef(summary(mod2))[, "Std. Error"]['medIncome'])
(coef(summary(mod2))[, 'Estimate']['medIncome']) - (qt(.975, 45)) * (coef(summary(mod2))[, "Std. Error"]['medIncome'])
```

```{r}
# Part 4
median_pop <- median(crimes$pop)
  # 94886
median_medIncome <- median(crimes$medIncome)
  # 27295.50
median_persHomeless <- median(crimes$persHomeless)
  # 7
median_racialMatch <- median(crimes$racialMatch)
  # 87.9
```

```{r}
# Part 5.1
predict(mod2, new=data.frame(pop=median_pop, 
                             medIncome=median_medIncome,
                             persHomeless = median_persHomeless,
                             racialMatch = median_racialMatch), 
        interval='prediction', level=.95)
```


```{r}
# Part 5.2
predict(mod2, new=data.frame(pop=median_pop, 
                             medIncome=median_medIncome,
                             persHomeless=median_persHomeless,
                             racialMatch=median_racialMatch), 
        interval='confidence', level=.95)
```

```{r}
# Part 5.3
predict(mod2, new=data.frame(pop=10000000, 
                             medIncome=median_medIncome,
                             persHomeless=median_persHomeless,
                             racialMatch=median_racialMatch), 
        interval='prediction', level=.95)
```
























